{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: pandas in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages (from scikit-learn) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk\n",
    "! pip install pandas\n",
    "! pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/guna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/guna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/guna/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/guna/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from string import punctuation\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def data_cleanup(train_df):\n",
    "    train_df['text'] = train_df['text'].str.lower()\n",
    "    train_df['text'] = train_df['text'].str.strip()\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='\\?*', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='(RT|rt)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='@[a-z,_]*', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='([0-9]*:[0-9]*)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='([0-9]*\\.[0-9]*)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='(utc|gmt)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='_[\\S]', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='&amp;?', value = 'and', regex = False)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='&lt;', value = '<', regex = False)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='&gt;', value = '>', regex = False)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='[ ]{2, }', value = ' ', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='([^\\w\\d ]+)', value = '', regex = True)\n",
    "    return train_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "train_df = pd.read_csv('Dataset/train.csv')\n",
    "train_df['text'] = data_cleanup(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this eahquake may ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the out of control wild fires in california ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m  5km s of volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating after an ebike collided w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the latest more homes razed by nohern californ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     our deeds are the reason of this eahquake may ...       1  \n",
       "1                 forest fire near la ronge sask canada       1  \n",
       "2     all residents asked to shelter in place are be...       1  \n",
       "3     13000 people receive wildfires evacuation orde...       1  \n",
       "4     just got sent this photo from ruby alaska as s...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  two giant cranes holding a bridge collapse int...       1  \n",
       "7609    the out of control wild fires in california ...       1  \n",
       "7610                        m  5km s of volcano hawaii        1  \n",
       "7611  police investigating after an ebike collided w...       1  \n",
       "7612  the latest more homes razed by nohern californ...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split training dataset\n",
    "tweet_texts = train_df['text']\n",
    "class_labels = train_df['target']\n",
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(tweet_texts,class_labels,test_size=0.2, random_state=42, stratify=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessor\n",
    "def preprocessing(text):\n",
    "   word_lemma = []\n",
    "   tweet_tokenize = TweetTokenizer()\n",
    "   tokens = tweet_tokenize.tokenize((text).lower())\n",
    "   tokens = [w for w in tokens if w not in punctuation and not w.isdigit() and not len(w) < 3]\n",
    "   stop_words = stopwords.words ('english')\n",
    "   tweet_without_stopwords = [t for t in tokens if t not in stop_words]\n",
    "   text = \" \".join (tweet_without_stopwords)\n",
    "   word_lemma = [WordNetLemmatizer().lemmatize(t) for t in tweet_tokenize.tokenize(text)]\n",
    "   pp_text = \" \".join (word_lemma)\n",
    "   return pp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_score(self, actual_label : list, predicted_label : list):\n",
    "    '''Function to calculate the performance metric using sklearn.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actual_label : list\n",
    "      Actual(Ground Truth) class label from the dataset.\n",
    "    predicted_label : pd.DataFrame\n",
    "      Class label predicted by the model\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    f1_score : float\n",
    "    accuracy : float\n",
    "    precision : float\n",
    "    recall : float\n",
    "    AUROC : float\n",
    "    '''\n",
    "    precision = metrics.precision_score(actual_label, predicted_label, pos_label=1)\n",
    "    recall = metrics.recall_score(actual_label, predicted_label,pos_label=1)\n",
    "    AUROC = metrics.roc_auc_score(actual_label, predicted_label)\n",
    "    accuracy = metrics.accuracy_score(actual_label, predicted_label)\n",
    "    f1_score = metrics.f1_score(actual_label, predicted_label,pos_label=1)\n",
    "    metrics_list = [f1_score, accuracy, precision, recall, AUROC]\n",
    "    metrics_list = pd.DataFrame(metrics_list).T\n",
    "    metrics_df = metrics_list.rename(columns={0:'F1',1:'Accuracy',2:'Precision',3:'Recall',4:'AUROC'})\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable definitions\n",
    " - train_tweets - Preprocessed tweets for training\n",
    " - test_tweets - Preprocessed tweets for testing\n",
    " - train_labels - class label for training tweets\n",
    " - test_labels - class label for test tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "1. Implement traditional model(MultinomialNB, LogisticRegression, SVC, KNeighborsClassifier) from sklearn\n",
    "2. Train and test the default model without tuning hyperparameter values\n",
    "3. Use grid search(GridSearchCV) from sklearn to identify best values for hyperparameters\n",
    "4. Train the model with best hypermeter values and test it on test set(test_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install numpy\n",
    "!pip install accelerate\n",
    "!pip install emoji==0.6.0\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertweetTokenizer\n",
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "from transformers import AutoModel\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
    "from transformers import AutoConfig\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in train_df.iterrows():\n",
    "        text = row['text']\n",
    "        pp_text = preprocessing(text)\n",
    "        train_df.at[index, 'text'] = pp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split training dataset\n",
    "tweet_texts = train_df['text']\n",
    "class_labels = train_df['target']\n",
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(tweet_texts,class_labels,test_size=0.2, random_state=42, stratify=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split training dataset\n",
    "tweet_texts = train_tweets\n",
    "class_labels = train_labels\n",
    "train_tweets, dev_tweets, train_labels, dev_labels = train_test_split(tweet_texts,class_labels,test_size=0.2, random_state=42, stratify=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = [pd.Series(train_tweets, name='text'), pd.Series(train_labels, name='labels')]\n",
    "train_df = pd.concat(train_cols, axis = 1)\n",
    "dev_cols = [pd.Series(dev_tweets, name='text'), pd.Series(dev_labels, name='labels')]\n",
    "dev_df = pd.concat(dev_cols, axis = 1)\n",
    "test_cols = [pd.Series(test_tweets, name='text'), pd.Series(test_labels, name='labels')]\n",
    "test_df = pd.concat(test_cols,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vinai/bertweet-base\"\n",
    "max_length = 32\n",
    "trucate = True\n",
    "padding='max_length'\n",
    "batch_size = 32\n",
    "mps_device = torch.device(\"mps\")\n",
    "id2text = {0: \"not_disaster\", 1: \"disaster\"}\n",
    "text2id = {\"not_disaster\": 0, \"disaster\": 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_tokenize(tweet_text):\n",
    "    tokenizer = BertweetTokenizer.from_pretrained(model_name)\n",
    "    IDs = tokenizer.encode_plus(\n",
    "                            tweet_text, \n",
    "                            add_special_tokens = True, \n",
    "                            max_length = max_length, \n",
    "                            padding = padding,\n",
    "                            truncation=trucate,\n",
    "                            return_attention_mask = True,\n",
    "                            return_tensors = 'pt'\n",
    "                            )\n",
    "    return IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(input_ids, attention_masks, token_type_ids, label):\n",
    "    map_dict = {}\n",
    "    map_dict['input_ids'] = input_ids\n",
    "    map_dict['token_type_ids'] = token_type_ids\n",
    "    map_dict['attention_mask'] = attention_masks\n",
    "    return map_dict, label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ds(input_df):\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "        \n",
    "    # for text, label in input_df:\n",
    "    for index, row in input_df.iterrows():\n",
    "        text = row['text']\n",
    "        label = row['labels']\n",
    "        classifier_input = tweet_tokenize(text)\n",
    "        input_ids_list.append(classifier_input['input_ids'])\n",
    "        token_type_ids_list.append(classifier_input['token_type_ids'])\n",
    "        attention_mask_list.append(classifier_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "    input_ids = torch.cat(input_ids_list, dim=0)\n",
    "    attention_masks = torch.cat(attention_mask_list, dim=0)\n",
    "    token_type_ids = torch.cat(token_type_ids_list, dim=0)\n",
    "    labels = torch.tensor(label_list)\n",
    "    mapped_dataset = TensorDataset(input_ids, attention_masks,token_type_ids, labels)\n",
    "    return mapped_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_mapped = build_ds(train_df.head(n=1000))\n",
    "dev_dataset_mapped = build_ds(dev_df.head(n=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertweetTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "classifier = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2text, label2id=text2id)\n",
    "classifier = classifier.to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(input):\n",
    "     token_ids_dict = tokenizer.encode_plus(input['text'], add_special_tokens = True, padding=padding, max_length=max_length, truncation=trucate,return_attention_mask = True)\n",
    "     token_ids_dict['label'] = input['labels']\n",
    "     # print(token_ids_dict)\n",
    "     return token_ids_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4872/4872 [00:00<00:00, 6551.95 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1218/1218 [00:00<00:00, 6085.57 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1523/1523 [00:00<00:00, 6030.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(dev_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "train_map = train_dataset.map(preprocessor)\n",
    "dev_map = eval_dataset.map(preprocessor)\n",
    "test_map = test_dataset.map(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = evaluate.load(\"f1\")\n",
    "def calculate_f1(labels):\n",
    "    predicted, actual = labels\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    return f1.compute(predictions=predicted, references=actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guna/anaconda3/envs/detect-disaster/lib/python3.12/site-packages/transformers/training_args.py:2046: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"trainer_cache\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model = 'f1',\n",
    "    greater_is_better=True,\n",
    "    num_train_epochs=8,\n",
    "    learning_rate = 1e-5,\n",
    "    adam_epsilon = 1e-5,\n",
    "    weight_decay = 1e-5,\n",
    "    adafactor = False,\n",
    "    use_mps_device=True\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_map,\n",
    "    eval_dataset=dev_map,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=calculate_f1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–         | 439/9135 [38:26<12:41:25,  5.25s/it]\n",
      " 10%|â–ˆ         | 501/4872 [01:22<11:54,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5093, 'grad_norm': 3.111316442489624, 'learning_rate': 8.973727422003284e-06, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–Ž        | 609/4872 [01:40<11:28,  6.19it/s]\n",
      " 12%|â–ˆâ–Ž        | 609/4872 [01:46<11:28,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4299132525920868, 'eval_f1': 0.7936507936507936, 'eval_runtime': 6.1221, 'eval_samples_per_second': 198.952, 'eval_steps_per_second': 24.992, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆ        | 1001/4872 [02:54<10:19,  6.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4411, 'grad_norm': 19.835969924926758, 'learning_rate': 7.947454844006569e-06, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 1218/4872 [03:30<09:46,  6.22it/s]\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 1218/4872 [03:34<09:46,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4644637107849121, 'eval_f1': 0.7913533834586466, 'eval_runtime': 4.0723, 'eval_samples_per_second': 299.091, 'eval_steps_per_second': 37.571, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆ       | 1501/4872 [04:22<09:06,  6.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3926, 'grad_norm': 2.905473232269287, 'learning_rate': 6.9211822660098524e-06, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1827/4872 [05:14<08:16,  6.14it/s]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1827/4872 [05:18<08:16,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.45878612995147705, 'eval_f1': 0.7947686116700201, 'eval_runtime': 4.0093, 'eval_samples_per_second': 303.79, 'eval_steps_per_second': 38.161, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2001/4872 [05:49<07:52,  6.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3785, 'grad_norm': 16.378162384033203, 'learning_rate': 5.894909688013136e-06, 'epoch': 3.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2436/4872 [06:59<06:35,  6.16it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2436/4872 [07:03<06:35,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.592811644077301, 'eval_f1': 0.7973609802073516, 'eval_runtime': 4.0789, 'eval_samples_per_second': 298.61, 'eval_steps_per_second': 37.51, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2501/4872 [07:16<06:25,  6.16it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3344, 'grad_norm': 0.3206090033054352, 'learning_rate': 4.868637110016421e-06, 'epoch': 4.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3001/4872 [08:38<05:09,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3096, 'grad_norm': 33.72257995605469, 'learning_rate': 3.842364532019705e-06, 'epoch': 4.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3045/4872 [08:45<04:56,  6.16it/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3045/4872 [08:49<04:56,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6568858027458191, 'eval_f1': 0.790009250693802, 'eval_runtime': 4.0438, 'eval_samples_per_second': 301.205, 'eval_steps_per_second': 37.836, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3501/4872 [10:05<03:42,  6.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2709, 'grad_norm': 26.653905868530273, 'learning_rate': 2.8160919540229887e-06, 'epoch': 5.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3654/4872 [10:29<03:21,  6.05it/s]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3654/4872 [10:33<03:21,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7191347479820251, 'eval_f1': 0.7928772258669166, 'eval_runtime': 4.0613, 'eval_samples_per_second': 299.905, 'eval_steps_per_second': 37.673, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4001/4872 [11:32<02:20,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2652, 'grad_norm': 13.423952102661133, 'learning_rate': 1.7898193760262728e-06, 'epoch': 6.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4263/4872 [12:14<01:38,  6.16it/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4263/4872 [12:18<01:38,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7972370386123657, 'eval_f1': 0.7864963503649635, 'eval_runtime': 4.0566, 'eval_samples_per_second': 300.25, 'eval_steps_per_second': 37.716, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4501/4872 [12:59<00:59,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2325, 'grad_norm': 0.1778857260942459, 'learning_rate': 7.635467980295568e-07, 'epoch': 7.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4872/4872 [13:59<00:00,  6.18it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4872/4872 [14:03<00:00,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7777602076530457, 'eval_f1': 0.7888372093023256, 'eval_runtime': 4.1646, 'eval_samples_per_second': 292.468, 'eval_steps_per_second': 36.739, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4872/4872 [14:06<00:00,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 846.0799, 'train_samples_per_second': 46.067, 'train_steps_per_second': 5.758, 'train_loss': 0.3411858719949456, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4872, training_loss=0.3411858719949456, metrics={'train_runtime': 846.0799, 'train_samples_per_second': 46.067, 'train_steps_per_second': 5.758, 'total_flos': 640938530856960.0, 'train_loss': 0.3411858719949456, 'epoch': 8.0})"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 191/191 [00:05<00:00, 36.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 0 0 0]\n",
      "\n",
      "\n",
      "confusion matric for learning rate: 3e-05\n",
      "\n",
      " [[729 140]\n",
      " [126 528]]\n",
      "\n",
      "\n",
      "Labelwise performance metrics for learning rate: 3e-05\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Student       0.85      0.84      0.85       869\n",
      "         LLM       0.79      0.81      0.80       654\n",
      "\n",
      "    accuracy                           0.83      1523\n",
      "   macro avg       0.82      0.82      0.82      1523\n",
      "weighted avg       0.83      0.83      0.83      1523\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.799, 0.823, 0.825, 0.79, 0.807)"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_label = test_df['labels']\n",
    "predictions_prob = trainer.predict(test_map)\n",
    "predictions =  predictions_prob.predictions\n",
    "predictions = np.argmax(predictions,axis=1)\n",
    "predicted_lables = np.array(predictions, dtype = int)\n",
    "actual_labels = np.array(actual_label, dtype = int)\n",
    "metrics_df = get_performance_score(actual_labels, predicted_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir = 'model/bertweet/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detect-disaster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
