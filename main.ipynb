{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nltk\n",
    "# %pip install pandas\n",
    "# %pip install scikit-learn\n",
    "# %pip install matplotlib\n",
    "# %pip install gradio\n",
    "# %pip install transformers\n",
    "# %pip install tensorflow\n",
    "# %pip install keras\n",
    "# %pip install Keras-Preprocessing\n",
    "# %pip install torch\n",
    "# %pip install datasets\n",
    "# %pip install evaluate\n",
    "# %pip install numpy\n",
    "# %pip install accelerate\n",
    "# %pip install emoji==0.6.0\n",
    "# %pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guna/anaconda3/envs/detect-disaster/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /Users/guna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/guna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/guna/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/guna/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gradio as gr\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import BertForSequenceClassification, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertweetTokenizer\n",
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoConfig\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.backend import clear_session\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from typing import Iterable\n",
    "from gradio.themes.base import Base\n",
    "from gradio.themes.utils import colors, fonts, sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_csv('dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def data_cleanup(train_df):\n",
    "    train_df['text'] = train_df['text'].str.lower()\n",
    "    train_df['text'] = train_df['text'].str.strip()\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='\\?*', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='(RT|rt)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='@[a-z,_]*', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='([0-9]*:[0-9]*)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='([0-9]*\\.[0-9]*)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='(utc|gmt)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='_[\\S]', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='&amp;?', value = 'and', regex = False)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='&lt;', value = '<', regex = False)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='&gt;', value = '>', regex = False)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='[ ]{2, }', value = ' ', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='([^\\w\\d ]+)', value = '', regex = True)\n",
    "    return train_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "input_df['text'] = data_cleanup(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this eahquake may ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the out of control wild fires in california ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m  5km s of volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating after an ebike collided w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the latest more homes razed by nohern californ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     our deeds are the reason of this eahquake may ...       1  \n",
       "1                 forest fire near la ronge sask canada       1  \n",
       "2     all residents asked to shelter in place are be...       1  \n",
       "3     13000 people receive wildfires evacuation orde...       1  \n",
       "4     just got sent this photo from ruby alaska as s...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  two giant cranes holding a bridge collapse int...       1  \n",
       "7609    the out of control wild fires in california ...       1  \n",
       "7610                        m  5km s of volcano hawaii        1  \n",
       "7611  police investigating after an ebike collided w...       1  \n",
       "7612  the latest more homes razed by nohern californ...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Test Dataset split\n",
    "tweet_texts = input_df['text']\n",
    "class_labels = input_df['target']\n",
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(tweet_texts,class_labels,test_size=0.2, random_state=42, stratify=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat tweets and labels series from the split into dataframe\n",
    "train_cols = [pd.Series(train_tweets, name='text'), pd.Series(train_labels, name='labels')]\n",
    "train_df = pd.concat(train_cols, axis = 1)\n",
    "test_cols = [pd.Series(test_tweets, name='text'), pd.Series(test_labels, name='labels')]\n",
    "test_df = pd.concat(test_cols,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessor\n",
    "def preprocessing(text):\n",
    "   word_lemma = []\n",
    "   tweet_tokenize = TweetTokenizer()\n",
    "   tokens = tweet_tokenize.tokenize((text).lower())\n",
    "   tokens = [w for w in tokens if w not in punctuation and not w.isdigit() and not len(w) < 3]\n",
    "   stop_words = stopwords.words ('english')\n",
    "   tweet_without_stopwords = [t for t in tokens if t not in stop_words]\n",
    "   text = \" \".join (tweet_without_stopwords)\n",
    "   word_lemma = [WordNetLemmatizer().lemmatize(t) for t in tweet_tokenize.tokenize(text)]\n",
    "   pp_text = \" \".join (word_lemma)\n",
    "   return pp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_score(actual_label : list, predicted_label : list):\n",
    "    '''Function to calculate the performance metric using sklearn.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actual_label : list\n",
    "      Actual(Ground Truth) class label from the dataset.\n",
    "    predicted_label : pd.DataFrame\n",
    "      Class label predicted by the model\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    f1_score : float\n",
    "    accuracy : float\n",
    "    precision : float\n",
    "    recall : float\n",
    "    AUROC : float\n",
    "    '''\n",
    "    precision = metrics.precision_score(actual_label, predicted_label, pos_label=1)\n",
    "    recall = metrics.recall_score(actual_label, predicted_label,pos_label=1)\n",
    "    AUROC = metrics.roc_auc_score(actual_label, predicted_label)\n",
    "    accuracy = metrics.accuracy_score(actual_label, predicted_label)\n",
    "    f1_score = metrics.f1_score(actual_label, predicted_label,pos_label=1)\n",
    "    confusion_mat = metrics.confusion_matrix(actual_label, predicted_label)\n",
    "    metrics_list = [accuracy, precision, recall, AUROC, f1_score]\n",
    "    metrics_list = pd.DataFrame(metrics_list).T\n",
    "    metrics_df = metrics_list.rename(columns={0:'Accuracy',1:'Precision',2:'Recall',3:'AUROC', 4:'F1'})\n",
    "    return metrics_df, confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion_mat, model_name):\n",
    "    _, cm_ax = plt.subplots(facecolor='#212936')\n",
    "    cm_plot = ConfusionMatrixDisplay(confusion_matrix=confusion_mat,display_labels=['Not Disaster','Disaster'])\n",
    "    title = model_name + \" Confusion Matrix\"\n",
    "    cm_plot.plot(cmap=plt.cm.Greens, ax=cm_ax)\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_perf_score(models_list, perf_score_list):\n",
    "  '''Function to consolidate the performance metrics of all the models(KNeighborsClassifier, RandomForestClassifier, LogisticRegression, MLPClassifier) \n",
    "  and return a pd.DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models_list : list\n",
    "      List of models.\n",
    "    perf_score_list : list\n",
    "      List of performance metrics data frame from various models.\n",
    "      \n",
    "    Return\n",
    "    ------\n",
    "    consolidated_metrics_df : pd.DataFrame\n",
    "    '''\n",
    "  \n",
    "  consolidated_perf_score_df = pd.concat(perf_score_list)\n",
    "  consolidated_perf_score_df = consolidated_perf_score_df.rename(columns={0:'Accuracy',1:'Precision',2:'Recall',3:'AUROC', 4:'F1'})\n",
    "  consolidated_perf_score_df.insert(0,'Model',models_list)\n",
    "  return consolidated_perf_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_plots(perf_results_df, themes = ['light', 'dark']):\n",
    "    model_dict = {'NB':'Naive Bayes', 'LR':'Logistic Regression','SVC':'SVM','KNC':'K-Nearest Neighbor',\n",
    "                'CNN':'CNN','RNN':'RNN',\n",
    "                'BERTweet':'BERTweet','RoBERTa':'RoBERTa'}\n",
    "    for theme in themes:\n",
    "        for shortName, longName in model_dict.items():\n",
    "            if theme == 'light':\n",
    "                plt.rcParams['text.color'] = 'black'\n",
    "                plt.rcParams['axes.labelcolor'] = 'black'\n",
    "                plt.rcParams['xtick.color'] = 'black'\n",
    "                plt.rcParams['ytick.color'] = 'black'\n",
    "                plt.rcParams['axes.edgecolor'] = 'black'\n",
    "                _, bar_ax = plt.subplots(facecolor='#FFFFFF')\n",
    "                bar_plot = perf_results_df[shortName].plot(figsize=(8,4), title=f\"{longName} Performance Metrics\", kind='bar', ax = bar_ax)\n",
    "                bar_plot.set_facecolor('#FFFFFF')\n",
    "            elif theme == 'dark':\n",
    "                plt.rcParams['text.color'] = 'white'\n",
    "                plt.rcParams['axes.labelcolor'] = 'white'\n",
    "                plt.rcParams['xtick.color'] = 'white'\n",
    "                plt.rcParams['ytick.color'] = 'white'\n",
    "                plt.rcParams['axes.edgecolor'] = '#ffffff'\n",
    "                _, bar_ax = plt.subplots(facecolor='#212936')\n",
    "                bar_plot = perf_results_df[shortName].plot(figsize=(8,4), title=f\"{longName} Performance Metrics\", kind='bar', ax=bar_ax)\n",
    "                bar_plot.set_facecolor('#212936')\n",
    "            bar_labels = bar_plot.bar(perf_results_df.index, perf_results_df[shortName], color = 'g', width=0.5)\n",
    "            bar_plot.bar_label(bar_labels, label_type='edge')\n",
    "            bar_plot.set_ylim([0, 1])\n",
    "            plt.savefig(f'results/images/performance/{longName}_{theme}.png', bbox_inches='tight')\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion_mat, theme, model):\n",
    "    if theme == 'dark':\n",
    "        plt.rcParams['text.color'] = 'white'\n",
    "        plt.rcParams['axes.labelcolor'] = 'white'\n",
    "        plt.rcParams['xtick.color'] = 'white'\n",
    "        plt.rcParams['ytick.color'] = 'white'\n",
    "        cm_fig, cm_ax = plt.subplots(facecolor='#212936', figsize=(6,4))\n",
    "    elif theme == 'light':\n",
    "        plt.rcParams['text.color'] = 'black'\n",
    "        plt.rcParams['axes.labelcolor'] = 'black'\n",
    "        plt.rcParams['xtick.color'] = 'black'\n",
    "        plt.rcParams['ytick.color'] = 'black'\n",
    "        cm_fig, cm_ax = plt.subplots(facecolor='#FFFFFF', figsize=(6,4))\n",
    "    cm_plot = ConfusionMatrixDisplay(confusion_matrix=confusion_mat,display_labels=['Not Disaster','Disaster'])\n",
    "    title = model + \" Confusion Matrix\"\n",
    "    cm_plot.plot(cmap=plt.cm.Greens, ax=cm_ax)\n",
    "    plt.title(title)\n",
    "    cm_fig.savefig(f'results/images/confusion/{model}_{theme}.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_all(perf_results_df, theme):\n",
    "    if theme == 'light':\n",
    "        plt.rcParams['text.color'] = 'black'\n",
    "        plt.rcParams['axes.labelcolor'] = 'black'\n",
    "        plt.rcParams['xtick.color'] = 'black'\n",
    "        plt.rcParams['ytick.color'] = 'black'\n",
    "        plt.rcParams['axes.edgecolor'] = 'black'\n",
    "        plt.rcParams['legend.facecolor'] = '#ffffff'\n",
    "        _, bar_ax = plt.subplots(facecolor='#FFFFFF')\n",
    "        bar_plot = perf_results_df.plot(figsize=(20,4), title=f\"Performance Metrics\", kind='bar', ax = bar_ax)\n",
    "        bar_plot.set_facecolor('#FFFFFF')\n",
    "    elif theme == 'dark':\n",
    "        plt.rcParams['text.color'] = 'white'\n",
    "        plt.rcParams['axes.labelcolor'] = 'white'\n",
    "        plt.rcParams['xtick.color'] = 'white'\n",
    "        plt.rcParams['ytick.color'] = 'white'\n",
    "        plt.rcParams['axes.edgecolor'] = '#ffffff'\n",
    "        plt.rcParams['legend.facecolor'] = '#212936'\n",
    "        _, bar_ax = plt.subplots(facecolor='#212936')\n",
    "        bar_plot = perf_results_df.plot(figsize=(20,4), title=f\"Performance Metrics\", kind='bar', ax=bar_ax, cmap=plt.cm.YlGn)\n",
    "        bar_plot.set_facecolor('#212936')\n",
    "    bar_ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    bar_plot.set_ylim([0, 1])\n",
    "    plt.savefig(f'results/images/performance/all_{theme}.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(preprocessor=preprocessing,ngram_range = (1,2))\n",
    "vectors_train = vectorizer.fit_transform(train_df['text'])\n",
    "vectors_test = vectorizer.transform(test_df['text'])\n",
    "train_labels = train_df['labels']\n",
    "test_labels = test_df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_grid_search(model, param_grid, cv, scoring, train_tweet, train_label):\n",
    "  '''Function to perform grid search.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Object\n",
    "    norm : str\n",
    "    param_grid : list\n",
    "    cv : int\n",
    "    scoring : str\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    grid_search : Object\n",
    "    '''\n",
    "  grid_search = GridSearchCV(model, param_grid, cv=cv, scoring=scoring, n_jobs=-1, return_train_score=True, error_score = True)\n",
    "  grid_search.fit(train_tweet, train_label)\n",
    "  return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_trad_models():\n",
    "    '''Function to initialize the traditional models from sklearn.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    nb : Object\n",
    "    knc : Object\n",
    "    lr : Object\n",
    "    svc : Object\n",
    "    '''\n",
    "    nb = MultinomialNB()\n",
    "    knc = KNeighborsClassifier()\n",
    "    lr = LogisticRegression()\n",
    "    svc = SVC()\n",
    "    return nb, knc,lr, svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb, knc, lr, svc = initialize_trad_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "param_grid = {\n",
    "      'alpha': (0.01,0.2,0.4,1.0),\n",
    "      'fit_prior': (True,False)}\n",
    "nb_gs = cv_grid_search(nb, param_grid, 10, 'f1', vectors_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'n_neighbors': [3, 5, 10, 12],\n",
    "               'p': [1, 2],\n",
    "               'weights':['uniform', 'distance'],\n",
    "               'algorithm': ['auto', 'brute']}]\n",
    "knc_gs = cv_grid_search(knc, param_grid, 10, 'f1', vectors_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'penalty': ['l2'],\n",
    "     'tol': [1e-3, 1e-4],\n",
    "     'solver':['lbfgs', 'liblinear'],\n",
    "     'max_iter': [1000, 5000, 10000],\n",
    "     'random_state': [42]}\n",
    "  ]\n",
    "lr_gs = cv_grid_search(lr, param_grid, 10, 'f1', vectors_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'kernel': ['poly', 'sigmoid', 'rbf'],\n",
    "     'gamma' : ['scale', 'auto'],\n",
    "     'random_state': [42]}\n",
    "  ]\n",
    "svc_gs = cv_grid_search(svc, param_grid, 10, 'f1', vectors_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nb, best_knc, best_lr, best_svc = initialize_trad_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best parameter values for each model from grid search for the hyperparameters \n",
    "best_nb.set_params(**nb_gs.best_params_)\n",
    "best_knc.set_params(**knc_gs.best_params_)\n",
    "best_lr.set_params(**lr_gs.best_params_)\n",
    "best_svc.set_params(**svc_gs.best_params_)\n",
    "\n",
    "#Train the each model with the best parameters\n",
    "best_nb.fit(vectors_train,train_labels)\n",
    "best_knc.fit(vectors_train,train_labels)\n",
    "best_lr.fit(vectors_train,train_labels)\n",
    "best_svc.fit(vectors_train,train_labels)\n",
    "\n",
    "#Predict the labels on test dataset using the trained models\n",
    "nb_predict = best_nb.predict(vectors_test)\n",
    "knc_predict = best_knc.predict(vectors_test)\n",
    "lr_predict = best_lr.predict(vectors_test)\n",
    "svc_predict = best_svc.predict(vectors_test)\n",
    "nb_predicted_labels = np.array(nb_predict, dtype = int)\n",
    "knc_predicted_labels = np.array(knc_predict, dtype = int)\n",
    "lr_predictted_labels = np.array(lr_predict, dtype = int)\n",
    "svc_predicted_labels = np.array(svc_predict, dtype = int)\n",
    "actual_labels = np.array(test_labels, dtype = int)\n",
    "\n",
    "#Calculate the performance metrics based on the predicted labels and actual labels in test dataset\n",
    "nb_perf_scores, nb_cm = get_performance_score(nb_predicted_labels, actual_labels)\n",
    "knc_perf_scores, knc_cm = get_performance_score(knc_predicted_labels, actual_labels)\n",
    "lr_perf_scores, lr_cm = get_performance_score(lr_predictted_labels, actual_labels)\n",
    "svc_perf_scores, svc_cm = get_performance_score(svc_predicted_labels, actual_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mats = {\n",
    "    'Naive Bayes': nb_cm,\n",
    "    'K-Nearest Neighbor': knc_cm,\n",
    "    'Logistic Regression': lr_cm,\n",
    "    'SVM': svc_cm\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_texts = input_df['text']\n",
    "class_labels = input_df['target']\n",
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(tweet_texts,class_labels,test_size=0.2, random_state=42, stratify=class_labels)\n",
    "train_tweets, validation_tweets, train_labels, validation_labels = train_test_split(train_tweets, train_labels, test_size = 0.2, random_state = 42, stratify=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets=train_tweets.to_frame()\n",
    "validation_tweets = validation_tweets.to_frame()\n",
    "test_tweets = test_tweets.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets['processed_text'] = train_tweets['text'].apply(preprocessing)\n",
    "validation_tweets['processed_text'] = validation_tweets['text'].apply(preprocessing)\n",
    "test_tweets['processed_text'] = test_tweets['text'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=18000)\n",
    "tokenizer.fit_on_texts(train_tweets['processed_text'])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_tweets['processed_text'])\n",
    "X_val = tokenizer.texts_to_sequences(validation_tweets['processed_text'])\n",
    "X_test = tokenizer.texts_to_sequences(test_tweets['processed_text'])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 128\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">44,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">114,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ predictions (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)       │     \u001b[38;5;34m1,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m44,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m114,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ predictions (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,176,385</span> (4.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,176,385\u001b[0m (4.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,176,385</span> (4.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,176,385\u001b[0m (4.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "cnn_layer = layers.Embedding(20000, embedding_dim)(inputs)\n",
    "cnn_layer = layers.Dropout(0.5)(cnn_layer)\n",
    "\n",
    "\n",
    "cnn_layer = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(cnn_layer)\n",
    "# cnn_layer = layers.Conv1D(128, 7, padding=\"valid\", activation=\"softmax\", strides=3)(cnn_layer)\n",
    "# cnn_layer = layers.Conv1D(128, 7, padding=\"valid\", activation=\"softmax\", strides=3)(cnn_layer)\n",
    "cnn_layer = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(cnn_layer)\n",
    "cnn_layer = layers.GlobalMaxPooling1D()(cnn_layer)\n",
    "\n",
    "\n",
    "cnn_layer = layers.Dense(128, activation=\"relu\")(cnn_layer)\n",
    "cnn_layer = layers.Dropout(0.5)(cnn_layer)\n",
    "\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(cnn_layer)\n",
    "\n",
    "cnn_model = keras.Model(inputs, predictions)\n",
    "\n",
    "cnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_model.fit(X_train, train_labels,\n",
    "#                     epochs=2,\n",
    "#                     verbose=False,\n",
    "#                     validation_data=(X_val, validation_labels),\n",
    "#                     batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_model.save('model/cnn/cnn_model.keras', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = keras.models.load_model('model/cnn/cnn_model.keras')\n",
    "cnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_test = cnn_model.predict(X_test)\n",
    "# pred_test = np.where(pred_test > 0.7, 1, 0)\n",
    "# pred_test = pred_test.flatten()\n",
    "# metrics_df_cnn, confusion_mat_cnn = get_performance_score(test_labels, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guna/anaconda3/envs/detect-disaster/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">536,250</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">510</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)       │       \u001b[38;5;34m536,250\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m510\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m110\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m110\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m110\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m110\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m11\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">537,211</span> (2.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m537,211\u001b[0m (2.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">537,211</span> (2.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m537,211\u001b[0m (2.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "rnn_layer = layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen)(inputs)\n",
    "rnn_layer = layers.Dropout(0.5)(rnn_layer)\n",
    "\n",
    "# model = Sequential()\n",
    "rnn_layer = layers.GlobalMaxPool1D()(rnn_layer)\n",
    "# model.add(layers.Flatten())\n",
    "rnn_layer = layers.Dense(10, activation='relu')(rnn_layer)\n",
    "rnn_layer = layers.Dense(10, activation='softmax')(rnn_layer)\n",
    "rnn_layer = layers.Dense(10, activation='tanh')(rnn_layer)\n",
    "rnn_layer = layers.Dense(10, activation='gelu')(rnn_layer)\n",
    "rnn_layer = layers.Dense(10, activation='relu')(rnn_layer)\n",
    "predictions = layers.Dense(1, activation='sigmoid')(rnn_layer)\n",
    "rnn_model = keras.Model(inputs, predictions)\n",
    "rnn_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn_model.fit(X_train, train_labels,\n",
    "#                     epochs=2,\n",
    "#                     verbose=False,\n",
    "#                     validation_data=(X_val, validation_labels),\n",
    "#                     batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn_model.save(\"model/rnn/rnn_model.keras\", include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guna/anaconda3/envs/detect-disaster/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 15 variables whereas the saved optimizer has 28 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "rnn_model = keras.models.load_model('model/rnn/rnn_model.keras')\n",
    "rnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_test = rnn_model.predict(X_test)\n",
    "# pred_test = np.where(pred_test > 0.7, 1, 0)\n",
    "# pred_test = pred_test.flatten()\n",
    "# metrics_df_rnn, confusion_mat_rnn = get_performance_score(test_labels, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidate_perf_score(['RNN', 'CNN'],[metrics_df_rnn, metrics_df_cnn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in input_df.iterrows():\n",
    "        text = row['text']\n",
    "        pp_text = preprocessing(text)\n",
    "        input_df.at[index, 'text'] = pp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Test Dataset split\n",
    "tweet_texts = input_df['text']\n",
    "class_labels = input_df['target']\n",
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(tweet_texts,class_labels,test_size=0.2, random_state=42, stratify=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##raining and Dev Dataset split\n",
    "tweet_texts = train_tweets\n",
    "class_labels = train_labels\n",
    "train_tweets, dev_tweets, train_labels, dev_labels = train_test_split(tweet_texts,class_labels,test_size=0.2, random_state=42, stratify=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat tweets and labels series from the split into dataframe\n",
    "train_cols = [pd.Series(train_tweets, name='text'), pd.Series(train_labels, name='labels')]\n",
    "train_df = pd.concat(train_cols, axis = 1)\n",
    "dev_cols = [pd.Series(dev_tweets, name='text'), pd.Series(dev_labels, name='labels')]\n",
    "dev_df = pd.concat(dev_cols, axis = 1)\n",
    "test_cols = [pd.Series(test_tweets, name='text'), pd.Series(test_labels, name='labels')]\n",
    "test_df = pd.concat(test_cols,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define constants for BERTweet model\n",
    "model_name = \"vinai/bertweet-base\"\n",
    "# model_name = \"model/bertweet/v1\"\n",
    "max_length = 32\n",
    "trucate = True\n",
    "padding='max_length'\n",
    "batch_size = 32\n",
    "id2text = {0: \"not_disaster\", 1: \"disaster\"}\n",
    "text2id = {\"not_disaster\": 0, \"disaster\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Intialize tokenizer, data_collector and classifier for BERTweet\n",
    "tokenizer = BertweetTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "classifier = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2text, label2id=text2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(input):\n",
    "     token_ids_dict = tokenizer.encode_plus(input['text'], add_special_tokens = True, padding=padding, max_length=max_length, truncation=trucate,return_attention_mask = True)\n",
    "     token_ids_dict['label'] = input['labels']\n",
    "     return token_ids_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4872/4872 [00:00<00:00, 7842.38 examples/s]\n",
      "Map: 100%|██████████| 1218/1218 [00:00<00:00, 6073.91 examples/s]\n",
      "Map: 100%|██████████| 1523/1523 [00:00<00:00, 6200.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#Convert the input text into token_ids, attention_mask and token_type_ids dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(dev_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "train_map = train_dataset.map(preprocessor)\n",
    "dev_map = eval_dataset.map(preprocessor)\n",
    "test_map = test_dataset.map(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(labels):\n",
    "    f1 = evaluate.load(\"accuracy\")\n",
    "    predicted, actual = labels\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    return f1.compute(predictions=predicted, references=actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Trainer and Training Arguments for finetuning BERTweet\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"trainer_cache\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model = 'accuracy',\n",
    "    greater_is_better=True,\n",
    "    num_train_epochs=8,\n",
    "    learning_rate = 1e-5,\n",
    "    adam_epsilon = 1e-5,\n",
    "    weight_decay = 1e-5,\n",
    "    adafactor = False,\n",
    "\n",
    ")\n",
    "\n",
    "bt_trainer = Trainer(\n",
    "    model=classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_map,\n",
    "    eval_dataset=dev_map,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=calculate_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [05:18<10:46,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3811, 'grad_norm': 9.013161659240723, 'learning_rate': 8.973727422003284e-06, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\u001b[A                                               \n",
      "\n",
      " 12%|█▎        | 609/4872 [05:40<10:46,  6.59it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5155385732650757, 'eval_accuracy': 0.825944170771757, 'eval_runtime': 4.7974, 'eval_samples_per_second': 253.886, 'eval_steps_per_second': 31.892, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [06:42<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3995, 'grad_norm': 46.756500244140625, 'learning_rate': 7.947454844006569e-06, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\u001b[A                                                \n",
      "\n",
      " 12%|█▎        | 609/4872 [07:20<10:46,  6.59it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4666399657726288, 'eval_accuracy': 0.8292282430213465, 'eval_runtime': 4.6475, 'eval_samples_per_second': 262.077, 'eval_steps_per_second': 32.921, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [08:05<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3633, 'grad_norm': 4.07218074798584, 'learning_rate': 6.9211822660098524e-06, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\u001b[A                                                \n",
      "\n",
      " 12%|█▎        | 609/4872 [08:59<10:46,  6.59it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5602150559425354, 'eval_accuracy': 0.8333333333333334, 'eval_runtime': 4.6927, 'eval_samples_per_second': 259.553, 'eval_steps_per_second': 32.604, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [09:29<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3362, 'grad_norm': 56.42657470703125, 'learning_rate': 5.894909688013136e-06, 'epoch': 3.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\u001b[A                                                \n",
      "\n",
      " 12%|█▎        | 609/4872 [10:40<10:46,  6.59it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7704036831855774, 'eval_accuracy': 0.8045977011494253, 'eval_runtime': 4.6564, 'eval_samples_per_second': 261.577, 'eval_steps_per_second': 32.858, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [10:53<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3002, 'grad_norm': 0.20008423924446106, 'learning_rate': 4.868637110016421e-06, 'epoch': 4.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [12:09<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2908, 'grad_norm': 0.23964877426624298, 'learning_rate': 3.842364532019705e-06, 'epoch': 4.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\u001b[A                                                \n",
      "\n",
      " 12%|█▎        | 609/4872 [12:21<10:46,  6.59it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.83719801902771, 'eval_accuracy': 0.8037766830870279, 'eval_runtime': 4.6289, 'eval_samples_per_second': 263.131, 'eval_steps_per_second': 33.053, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [13:33<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2338, 'grad_norm': 18.457305908203125, 'learning_rate': 2.8160919540229887e-06, 'epoch': 5.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\u001b[A                                                \n",
      "\n",
      " 12%|█▎        | 609/4872 [14:01<10:46,  6.59it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8814917802810669, 'eval_accuracy': 0.7980295566502463, 'eval_runtime': 4.8396, 'eval_samples_per_second': 251.673, 'eval_steps_per_second': 31.614, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [14:57<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2261, 'grad_norm': 0.3064159154891968, 'learning_rate': 1.7898193760262728e-06, 'epoch': 6.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\u001b[A                                                \n",
      "\n",
      " 12%|█▎        | 609/4872 [15:41<10:46,  6.59it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9191906452178955, 'eval_accuracy': 0.7980295566502463, 'eval_runtime': 4.6142, 'eval_samples_per_second': 263.97, 'eval_steps_per_second': 33.159, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [16:21<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1923, 'grad_norm': 0.14674822986125946, 'learning_rate': 7.635467980295568e-07, 'epoch': 7.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\u001b[A                                                \n",
      "\n",
      " 12%|█▎        | 609/4872 [17:22<10:46,  6.59it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8857409954071045, 'eval_accuracy': 0.8119868637110016, 'eval_runtime': 4.6332, 'eval_samples_per_second': 262.883, 'eval_steps_per_second': 33.022, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "100%|██████████| 4872/4872 [13:23<00:00,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 803.9221, 'train_samples_per_second': 48.482, 'train_steps_per_second': 6.06, 'train_loss': 0.29546034394813875, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4872, training_loss=0.29546034394813875, metrics={'train_runtime': 803.9221, 'train_samples_per_second': 48.482, 'train_steps_per_second': 6.06, 'total_flos': 640938530856960.0, 'train_loss': 0.29546034394813875, 'epoch': 8.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finetune BERTweet\n",
    "bt_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:06<00:00, 31.22it/s]\n"
     ]
    }
   ],
   "source": [
    "actual_label = test_df['labels']\n",
    "predictions_prob = bt_trainer.predict(test_map)\n",
    "predictions =  predictions_prob.predictions\n",
    "predictions = np.argmax(predictions,axis=1)\n",
    "predicted_lables = np.array(predictions, dtype = int)\n",
    "actual_labels = np.array(actual_label, dtype = int)\n",
    "bt_metrics_df, bt_confusion_mat = get_performance_score(actual_labels, predicted_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mats['BERTweet'] = bt_confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_trainer.save_model(output_dir = 'model/bertweet/v2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"FacebookAI/xlm-roberta-base\"\n",
    "# model_name = 'model/roberta/v1'\n",
    "max_length = 32\n",
    "trucate = True\n",
    "padding='max_length'\n",
    "id2text = {0: \"not_disaster\", 1: \"disaster\"}\n",
    "text2id = {\"not_disaster\": 0, \"disaster\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "classifier = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2text, label2id=text2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(input):\n",
    "     token_ids_dict = tokenizer.encode_plus(input['text'], add_special_tokens = True, padding=padding, max_length=max_length, truncation=trucate,return_attention_mask = True)\n",
    "     token_ids_dict['label'] = input['labels']\n",
    "     return token_ids_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4872/4872 [00:00<00:00, 7575.79 examples/s]\n",
      "Map: 100%|██████████| 1218/1218 [00:00<00:00, 9968.74 examples/s] \n",
      "Map: 100%|██████████| 1523/1523 [00:00<00:00, 9820.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#Convert the input text into token_ids, attention_mask and token_type_ids dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(dev_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "train_map = train_dataset.map(preprocessor)\n",
    "dev_map = eval_dataset.map(preprocessor)\n",
    "test_map = test_dataset.map(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"trainer_cache\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model = 'accuracy',\n",
    "    greater_is_better=True,\n",
    "    num_train_epochs=8,\n",
    "    learning_rate = 1e-5,\n",
    "    adam_epsilon = 1e-5,\n",
    "    weight_decay = 1e-5,\n",
    "    adafactor = False,\n",
    "    use_mps_device=False\n",
    "\n",
    ")\n",
    "\n",
    "rb_trainer = Trainer(\n",
    "    model=classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_map,\n",
    "    eval_dataset=dev_map,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=calculate_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [19:37<10:46,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5741, 'grad_norm': 9.048018455505371, 'learning_rate': 8.973727422003284e-06, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      " 12%|█▎        | 609/4872 [20:07<10:46,  6.59it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5399500727653503, 'eval_accuracy': 0.7430213464696224, 'eval_runtime': 4.7979, 'eval_samples_per_second': 253.864, 'eval_steps_per_second': 31.889, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [21:42<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5034, 'grad_norm': 29.211271286010742, 'learning_rate': 7.947454844006569e-06, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      " 12%|█▎        | 609/4872 [22:38<10:46,  6.59it/s] \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.573973536491394, 'eval_accuracy': 0.777504105090312, 'eval_runtime': 4.6747, 'eval_samples_per_second': 260.549, 'eval_steps_per_second': 32.729, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [23:48<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.437, 'grad_norm': 11.050125122070312, 'learning_rate': 6.9211822660098524e-06, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      " 12%|█▎        | 609/4872 [25:08<10:46,  6.59it/s] \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46417587995529175, 'eval_accuracy': 0.8284072249589491, 'eval_runtime': 4.9044, 'eval_samples_per_second': 248.348, 'eval_steps_per_second': 31.196, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [25:53<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4239, 'grad_norm': 90.51426696777344, 'learning_rate': 5.894909688013136e-06, 'epoch': 3.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      " 12%|█▎        | 609/4872 [27:39<10:46,  6.59it/s] \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6456968188285828, 'eval_accuracy': 0.7832512315270936, 'eval_runtime': 4.7505, 'eval_samples_per_second': 256.394, 'eval_steps_per_second': 32.207, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [27:59<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4107, 'grad_norm': 5.348498821258545, 'learning_rate': 4.868637110016421e-06, 'epoch': 4.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [29:55<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4021, 'grad_norm': 17.72875213623047, 'learning_rate': 3.842364532019705e-06, 'epoch': 4.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      " 12%|█▎        | 609/4872 [30:10<10:46,  6.59it/s] \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5385130643844604, 'eval_accuracy': 0.819376026272578, 'eval_runtime': 4.5661, 'eval_samples_per_second': 266.751, 'eval_steps_per_second': 33.508, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [32:01<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3724, 'grad_norm': 78.7192611694336, 'learning_rate': 2.8160919540229887e-06, 'epoch': 5.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      " 12%|█▎        | 609/4872 [32:42<10:46,  6.59it/s] \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7128706574440002, 'eval_accuracy': 0.8111658456486043, 'eval_runtime': 4.7692, 'eval_samples_per_second': 255.388, 'eval_steps_per_second': 32.081, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [34:08<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3671, 'grad_norm': 69.55010986328125, 'learning_rate': 1.7898193760262728e-06, 'epoch': 6.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      " 12%|█▎        | 609/4872 [35:14<10:46,  6.59it/s] \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6669760346412659, 'eval_accuracy': 0.8185550082101807, 'eval_runtime': 4.7752, 'eval_samples_per_second': 255.065, 'eval_steps_per_second': 32.04, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▎        | 609/4872 [36:14<10:46,  6.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3341, 'grad_norm': 72.45380401611328, 'learning_rate': 7.635467980295568e-07, 'epoch': 7.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      " 12%|█▎        | 609/4872 [37:45<10:46,  6.59it/s] \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.679907500743866, 'eval_accuracy': 0.8226600985221675, 'eval_runtime': 4.7157, 'eval_samples_per_second': 258.289, 'eval_steps_per_second': 32.445, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "100%|██████████| 4872/4872 [20:14<00:00,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1214.8889, 'train_samples_per_second': 32.082, 'train_steps_per_second': 4.01, 'train_loss': 0.4203346734759451, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4872, training_loss=0.4203346734759451, metrics={'train_runtime': 1214.8889, 'train_samples_per_second': 32.082, 'train_steps_per_second': 4.01, 'total_flos': 640938530856960.0, 'train_loss': 0.4203346734759451, 'epoch': 8.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:05<00:00, 33.92it/s]\n"
     ]
    }
   ],
   "source": [
    "actual_label = test_df['labels']\n",
    "predictions_prob = rb_trainer.predict(test_map)\n",
    "predictions =  predictions_prob.predictions\n",
    "predictions = np.argmax(predictions,axis=1)\n",
    "predicted_lables = np.array(predictions, dtype = int)\n",
    "actual_labels = np.array(actual_label, dtype = int)\n",
    "rb_metrics_df, rb_confusion_mat = get_performance_score(actual_labels, predicted_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_trainer.save_model(output_dir = 'model/roberta/v2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mats['RoBERTa'] = rb_confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>AUROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.766400</td>\n",
       "      <td>0.808273</td>\n",
       "      <td>0.732416</td>\n",
       "      <td>0.803691</td>\n",
       "      <td>0.807455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.621535</td>\n",
       "      <td>0.533815</td>\n",
       "      <td>0.891437</td>\n",
       "      <td>0.477087</td>\n",
       "      <td>0.620603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.773289</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.717125</td>\n",
       "      <td>0.838998</td>\n",
       "      <td>0.823545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.761578</td>\n",
       "      <td>0.817466</td>\n",
       "      <td>0.678899</td>\n",
       "      <td>0.867188</td>\n",
       "      <td>0.829736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERTweet</td>\n",
       "      <td>0.804416</td>\n",
       "      <td>0.837163</td>\n",
       "      <td>0.830619</td>\n",
       "      <td>0.779817</td>\n",
       "      <td>0.830069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RoBERTa</td>\n",
       "      <td>0.796923</td>\n",
       "      <td>0.826658</td>\n",
       "      <td>0.801858</td>\n",
       "      <td>0.792049</td>\n",
       "      <td>0.822377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model        F1  Accuracy  Precision    Recall     AUROC\n",
       "0        NB  0.766400  0.808273   0.732416  0.803691  0.807455\n",
       "0       KNN  0.621535  0.533815   0.891437  0.477087  0.620603\n",
       "0        LR  0.773289  0.819435   0.717125  0.838998  0.823545\n",
       "0       SVM  0.761578  0.817466   0.678899  0.867188  0.829736\n",
       "0  BERTweet  0.804416  0.837163   0.830619  0.779817  0.830069\n",
       "0   RoBERTa  0.796923  0.826658   0.801858  0.792049  0.822377"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consolidate_perf_score(['NB', 'KNN', 'LR', 'SVM', 'CNN', 'RNN', 'BERTweet', 'RoBERTa'], [nb_perf_scores, knc_perf_scores, lr_perf_scores, svc_perf_scores, metrics_df_cnn, metrics_df_rnn, bt_metrics_df, rb_metrics_df])\n",
    "consolidate_perf_score(['NB', 'KNN', 'LR', 'SVM', 'BERTweet', 'RoBERTa'], [nb_perf_scores, knc_perf_scores, lr_perf_scores, svc_perf_scores, bt_metrics_df, rb_metrics_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_mats(confusion_mats, themes = ['light', 'dark']):\n",
    "    for theme in themes:\n",
    "        for name, confusion_mat in confusion_mats.items():\n",
    "            plot_confusion_matrix(confusion_mat, theme, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Trainer and Training Arguments for finetuning BERTweet\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"trainer_cache\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model = 'f1',\n",
    "    greater_is_better=True,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate = 1e-5,\n",
    "    adam_epsilon = 1e-5,\n",
    "    weight_decay = 1e-5,\n",
    "    adafactor = False,\n",
    "\n",
    ")\n",
    "\n",
    "bt_trainer = Trainer(\n",
    "    model=classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_map,\n",
    "    eval_dataset=dev_map,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=calculate_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod_preprocessor(input):\n",
    "     token_ids_dict = tokenizer.encode_plus(input['text'], add_special_tokens = True, padding=padding, max_length=max_length, truncation=trucate,return_attention_mask = True)\n",
    "     return token_ids_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_results_df = pd.read_csv('results/csv/performance_results.csv')\n",
    "# perf_results_df = perf_results_df.T\n",
    "# perf_results_df.columns=['NB', 'KNC', 'LR', 'SVC', 'BERTweet', 'RoBERTa', 'RNN', 'CNN']\n",
    "# perf_results_df=perf_results_df[1:]\n",
    "# perf_results_df = perf_results_df.astype('float32')\n",
    "# perf_results_df = perf_results_df.round(2)\n",
    "# perf_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_confusion_mats(confusion_mats, themes=['dark'])\n",
    "# plot_bar_plots(perf_results_df, themes=['dark'])\n",
    "# plot_bar_all(perf_results_df, themes=['dark'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod_file_predict(path):\n",
    "    prod_input_df = pd.read_csv(path)\n",
    "    prod_input_display = pd.DataFrame()\n",
    "    prod_input_display['Tweet Text'] = prod_input_df['text']\n",
    "    prod_input_df['text'] = data_cleanup(prod_input_df)\n",
    "    prod_input_df['text'] = prod_input_df['text'].apply(preprocessing)\n",
    "    prod_test_dataset = Dataset.from_pandas(prod_input_df)\n",
    "    prod_test_dataset_map = prod_test_dataset.map(prod_preprocessor)\n",
    "    prod_predictions_prob = bt_trainer.predict(prod_test_dataset_map)\n",
    "    prod_predictions =  prod_predictions_prob.predictions\n",
    "    prod_predictions = np.argmax(prod_predictions,axis=1)\n",
    "    prod_predicted_lables = np.array(prod_predictions, dtype = int)\n",
    "    prod_output_df = pd.DataFrame(data=prod_input_df['text'], columns=['text'])\n",
    "    prod_output_df['label'] = pd.Series(prod_predicted_lables)\n",
    "    output_map = {\n",
    "        0: 'Not Disaster',\n",
    "        1: 'Disaster'\n",
    "    }\n",
    "    prod_output_df['label'] = prod_output_df['label'].map(output_map)\n",
    "    prod_input_display['Class Label'] = prod_output_df['label']\n",
    "    return prod_input_display\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod_text_predict(text):\n",
    "    text_dict = {}\n",
    "    text_dict['text'] = text\n",
    "    prod_input_df = pd.DataFrame(text_dict.values(), columns=['text'])\n",
    "    prod_input_df['text'] = data_cleanup(prod_input_df)\n",
    "    prod_input_df['text'] = prod_input_df['text'].apply(preprocessing)\n",
    "    prod_test_dataset = Dataset.from_pandas(prod_input_df)\n",
    "    prod_test_dataset_map = prod_test_dataset.map(prod_preprocessor)\n",
    "    prod_predictions_prob = bt_trainer.predict(prod_test_dataset_map)\n",
    "    prod_predictions =  prod_predictions_prob.predictions\n",
    "    prod_predictions = np.argmax(prod_predictions,axis=1)\n",
    "    prod_predicted_lables = np.array(prod_predictions, dtype = int)\n",
    "    prod_output_df = pd.DataFrame(data=prod_input_df['text'], columns=['text'])\n",
    "    prod_output_df['label'] = pd.Series(prod_predicted_lables)\n",
    "    output_map = {\n",
    "        0: 'Not Disaster',\n",
    "        1: 'Disaster'\n",
    "    }\n",
    "    prod_output_df['label'] = prod_output_df['label'].map(output_map)\n",
    "    return prod_output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(metrics_input,model_input):\n",
    "    theme = 'dark'\n",
    "    metric_dict = {'Performance Metrics':'./results/images/performance/', \n",
    "               'Confusion Matrix':'./results/images/confusion/'}\n",
    "    model_dict = {'NB':'Naive Bayes', 'LR':'Logistic Regression','SVM':'SVM','KNN':'K-Nearest Neighbor',\n",
    "              'CNN':'CNN','RNN':'RNN',\n",
    "              'BERTweet':'BERTweet','RoBERTa':'RoBERTa'}\n",
    "    base_dir = metric_dict[metrics_input]\n",
    "    filename = model_dict[model_input]\n",
    "    file_path = base_dir+filename+'_'+theme+'.png'\n",
    "    return file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customtheme(Base):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        primary_hue: colors.Color | str = colors.green,\n",
    "        secondary_hue: colors.Color | str = colors.green,\n",
    "        neutral_hue: colors.Color | str = colors.gray,\n",
    "        spacing_size: sizes.Size | str = sizes.spacing_sm,\n",
    "        radius_size: sizes.Size | str = sizes.radius_lg,\n",
    "        text_size: sizes.Size | str = sizes.text_lg,\n",
    "        font: fonts.Font\n",
    "        | str\n",
    "        | Iterable[fonts.Font | str] = (\n",
    "            fonts.GoogleFont(\"Quicksand\"),\n",
    "            \"ui-sans-serif\",\n",
    "            \"sans-serif\",\n",
    "        ),\n",
    "        font_mono: fonts.Font\n",
    "        | str\n",
    "        | Iterable[fonts.Font | str] = (\n",
    "            fonts.GoogleFont(\"IBM Plex Mono\"),\n",
    "            \"ui-monospace\",\n",
    "            \"monospace\",\n",
    "        ),\n",
    "    ):\n",
    "        super().__init__(\n",
    "            primary_hue=primary_hue,\n",
    "            secondary_hue=secondary_hue,\n",
    "            neutral_hue=neutral_hue,\n",
    "            spacing_size=spacing_size,\n",
    "            radius_size=radius_size,\n",
    "            text_size=text_size,\n",
    "            font=font,\n",
    "            font_mono=font_mono,\n",
    "        )\n",
    "        super().set(\n",
    "            block_title_text_weight=\"300\",\n",
    "            block_border_width=\"3px\",\n",
    "            block_shadow=\"*shadow_drop_lg\",\n",
    "            button_shadow=\"*shadow_drop_lg\",\n",
    "            button_large_padding=\"12px\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_predict(input):\n",
    "    output = prod_text_predict(input)\n",
    "    if output.iloc[0]['label'] == \"Disaster\":\n",
    "        output_statement = \"This text is related to disaster\"\n",
    "    if output.iloc[0]['label'] == \"Not Disaster\":\n",
    "        output_statement = \"This text is not related to disaster\"\n",
    "    return output_statement\n",
    "\n",
    "def file_predict(input, progress=gr.Progress(track_tqdm=True)):\n",
    "    output = prod_file_predict(input)\n",
    "    return output\n",
    "\n",
    "def api_monitor(input):\n",
    "    output = \"This feature is yet to be implemented\"\n",
    "    return output\n",
    "\n",
    "def model_performance(metrics_input,model_input):\n",
    "    file_path = get_file_name(metrics_input,model_input)\n",
    "    output_plot = gr.Image(file_path, height = 600, width = 600)\n",
    "    return output_plot\n",
    "\n",
    "def clear(text_input, text_output):\n",
    "    text_input = None\n",
    "    text_output = None\n",
    "    return text_input, text_output\n",
    "\n",
    "customtheme_obj = customtheme()\n",
    "with gr.Blocks(theme=customtheme_obj) as gui_demo:\n",
    "    gr.Markdown(\"Automated Classification of Disaster-Related Tweets\")\n",
    "    with gr.Tab(\"Text\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                text_input = gr.Textbox(label= \"Input Tweet\")\n",
    "                with gr.Row():\n",
    "                    text_button = gr.Button(\"Predict\", variant='primary')\n",
    "                    text_clear_button = gr.ClearButton([text_input])\n",
    "            with gr.Column():\n",
    "                text_output = gr.Textbox(label = \"Prediction\")\n",
    "        \n",
    "    with gr.Tab(\"File\"):\n",
    "        # with gr.Row():\n",
    "        file_input = gr.File(label= \"Input File (as .csv)\")\n",
    "        default_display_df = pd.DataFrame(index = range(1), columns=['Tweet Text', 'Class Label'])\n",
    "        file_output = gr.DataFrame(value = default_display_df, label = \"Prediction\")\n",
    "        file_button = gr.Button(\"Predict\", variant='primary')\n",
    "    with gr.Tab(\"API\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                api_input = gr.Textbox(label= \"API Endpoint\")\n",
    "                with gr.Row():\n",
    "                    api_button = gr.Button(\"Monitor\", variant='primary')\n",
    "                    api_clear_button = gr.ClearButton([api_input])\n",
    "            with gr.Column():\n",
    "                api_output = gr.Textbox(label = \"Disaster Related Tweets\")\n",
    "    with gr.Tab(\"Model Performance\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                metric_choices = ['Performance Metrics', 'Confusion Matrix']\n",
    "                metrics_input = gr.Radio(choices = metric_choices, \n",
    "                                        value = 'Performance Metrics', \n",
    "                                        type = 'value',\n",
    "                                        show_label = True, \n",
    "                                        interactive = True,\n",
    "                                        label= \"Select metric\")\n",
    "                model_choices = ['NB', 'LR', 'SVM', 'KNN', 'CNN', 'RNN', 'BERTweet', 'RoBERTa', 'All' ]\n",
    "                model_input = gr.Radio(choices = model_choices, \n",
    "                                        value = 'NB', \n",
    "                                        type = 'value',\n",
    "                                        show_label = True, \n",
    "                                        interactive = True,\n",
    "                                        label= \"Select model\")\n",
    "                model_perf_button = gr.Button(\"View Results\", variant='primary')\n",
    "            with gr.Column():\n",
    "                model_output = gr.Image()\n",
    "       \n",
    "\n",
    "    text_button.click(text_predict, inputs=text_input, outputs=text_output)\n",
    "    file_button.click(file_predict, inputs=file_input, outputs=file_output)\n",
    "    api_button.click(api_monitor, inputs=api_input, outputs=api_output)\n",
    "    model_perf_button.click(model_performance, inputs=[metrics_input,model_input] , outputs=model_output)\n",
    "\n",
    "gui_demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "gui_demo.close()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detect-disaster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
