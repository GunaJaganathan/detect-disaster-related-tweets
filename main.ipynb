{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk\n",
    "! pip install pandas\n",
    "! pip install scikit-learn\n",
    "! pip install tensorflow[and-cuda]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from string import punctuation\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "def data_cleanup(train_df):\n",
    "    train_df['text'] = train_df['text'].str.lower()\n",
    "    train_df['text'] = train_df['text'].str.strip()\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='\\?*', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='(RT|rt)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='@[a-z,_]*', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='([0-9]*:[0-9]*)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='([0-9]*\\.[0-9]*)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='(utc|gmt)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='_[\\S]', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='&amp;?', value = 'and', regex = False)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='&lt;', value = '<', regex = False)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='&gt;', value = '>', regex = False)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='[ ]{2, }', value = ' ', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='([^\\w\\d ]+)', value = '', regex = True)\n",
    "    return train_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "train_df['text'] = data_cleanup(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split training dataset\n",
    "tweet_texts = train_df['text']\n",
    "class_labels = train_df['target']\n",
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(tweet_texts,class_labels,test_size=0.2, random_state=42, stratify=class_labels)\n",
    "train_tweets, validation_tweets, train_labels, validation_labels = train_test_split(train_tweets, train_labels, test_size = 0.2, random_state = 42, stratify=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tweets = train_tweets.str.cat().lower()\n",
    "# validation_tweets = validation_tweets.str.cat().lower()\n",
    "# test_tweets = test_tweets.str.cat().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessor\n",
    "def preprocessing(text):\n",
    "   word_lemma = []\n",
    "   tweet_tokenize = TweetTokenizer()\n",
    "   tokens = tweet_tokenize.tokenize((text).lower())\n",
    "   tokens = [w for w in tokens if w not in punctuation and not w.isdigit() and not len(w) < 3]\n",
    "   stop_words = stopwords.words ('english')\n",
    "   tweet_without_stopwords = [t for t in tokens if t not in stop_words]\n",
    "   text = \" \".join (tweet_without_stopwords)\n",
    "   word_lemma = [WordNetLemmatizer().lemmatize(t) for t in tweet_tokenize.tokenize(text)]\n",
    "   pp_text = \" \".join (word_lemma)\n",
    "   return pp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_score(self, actual_label : list, predicted_label : list):\n",
    "    '''Function to calculate the performance metric using sklearn.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actual_label : list\n",
    "      Actual(Ground Truth) class label from the dataset.\n",
    "    predicted_label : pd.DataFrame\n",
    "      Class label predicted by the model\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    f1_score : float\n",
    "    accuracy : float\n",
    "    precision : float\n",
    "    recall : float\n",
    "    AUROC : float\n",
    "    '''\n",
    "    precision = metrics.precision_score(actual_label, predicted_label, pos_label=1)\n",
    "    recall = metrics.recall_score(actual_label, predicted_label,pos_label=1)\n",
    "    AUROC = metrics.roc_auc_score(actual_label, predicted_label)\n",
    "    accuracy = metrics.accuracy_score(actual_label, predicted_label)\n",
    "    f1_score = metrics.f1_score(actual_label, predicted_label,pos_label=1)\n",
    "    metrics_list = [f1_score, accuracy, precision, recall, AUROC]\n",
    "    metrics_list = pd.DataFrame(metrics_list).T\n",
    "    metrics_df = metrics_list.rename(columns={0:'F1',1:'Accuracy',2:'Precision',3:'Recall',4:'AUROC'})\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "# train_tweets = preprocessing(train_tweets)\n",
    "# validation_tweets = preprocessing(validation_tweets)\n",
    "# test_tweets = preprocessing(test_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable definitions\n",
    " - train_tweets - Preprocessed tweets for training\n",
    " - test_tweets - Preprocessed tweets for testing\n",
    " - train_labels - class label for training tweets\n",
    " - test_labels - class label for test tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "1. Implement traditional model(MultinomialNB, LogisticRegression, SVC, KNeighborsClassifier) from sklearn\n",
    "2. Train and test the default model without tuning hyperparameter values\n",
    "3. Use grid search(GridSearchCV) from sklearn to identify best values for hyperparameters\n",
    "4. Train the model with best hypermeter values and test it on test set(test_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(train_df['text'].str.split().apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets=train_tweets.to_frame()\n",
    "validation_tweets = validation_tweets.to_frame()\n",
    "test_tweets = test_tweets.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets['processed_text'] = train_tweets['text'].apply(preprocessing)\n",
    "validation_tweets['processed_text'] = validation_tweets['text'].apply(preprocessing)\n",
    "test_tweets['processed_text'] = test_tweets['text'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_tweets['processed_text'])\n",
    "\n",
    "X_train = vectorizer.transform(train_tweets['processed_text'])\n",
    "X_val  = vectorizer.transform(validation_tweets['processed_text'])\n",
    "X_test = vectorizer.transform(test_tweets['processed_text'])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, train_labels)\n",
    "score = classifier.score(X_val, validation_labels)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.toarray()\n",
    "X_val = X_val.toarray()\n",
    "X_test = X_test.toarray()\n",
    "history = model.fit(X_train, train_labels,\n",
    "                    epochs=100,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_val, validation_labels),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_train, train_labels, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_val, validation_labels, verbose=False)\n",
    "print(\"Validation Accuracy:  {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, test_labels, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
