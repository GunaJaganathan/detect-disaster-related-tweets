{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (1.4.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (2.13.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.1 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.1->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: keras in c:\\users\\viswa\\miniconda3\\envs\\mlproject\\lib\\site-packages (2.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install tensorflow\n",
    "%pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\viswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\viswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\viswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\viswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from string import punctuation\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.backend import clear_session\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "def data_cleanup(train_df):\n",
    "    train_df['text'] = train_df['text'].str.lower()\n",
    "    train_df['text'] = train_df['text'].str.strip()\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='\\?*', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='(RT|rt)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='@[a-z,_]*', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='([0-9]*:[0-9]*)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='([0-9]*\\.[0-9]*)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='(utc|gmt)', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='_[\\S]', value = '', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='&amp;?', value = 'and', regex = False)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='&lt;', value = '<', regex = False)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='&gt;', value = '>', regex = False)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='[ ]{2, }', value = ' ', regex = True)\n",
    "    train_df['text'] = train_df['text'].replace(to_replace ='([^\\w\\d ]+)', value = '', regex = True)\n",
    "    return train_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "train_df['text'] = data_cleanup(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split training dataset\n",
    "tweet_texts = train_df['text']\n",
    "class_labels = train_df['target']\n",
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(tweet_texts,class_labels,test_size=0.2, random_state=42, stratify=class_labels)\n",
    "train_tweets, validation_tweets, train_labels, validation_labels = train_test_split(train_tweets, train_labels, test_size = 0.2, random_state = 42, stratify=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessor\n",
    "def preprocessing(text):\n",
    "   word_lemma = []\n",
    "   tweet_tokenize = TweetTokenizer()\n",
    "   tokens = tweet_tokenize.tokenize((text).lower())\n",
    "   tokens = [w for w in tokens if w not in punctuation and not w.isdigit() and not len(w) < 3]\n",
    "   stop_words = stopwords.words ('english')\n",
    "   tweet_without_stopwords = [t for t in tokens if t not in stop_words]\n",
    "   text = \" \".join (tweet_without_stopwords)\n",
    "   word_lemma = [WordNetLemmatizer().lemmatize(t) for t in tweet_tokenize.tokenize(text)]\n",
    "   pp_text = \" \".join (word_lemma)\n",
    "   return pp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_score(actual_label : list, predicted_label : list):\n",
    "    '''Function to calculate the performance metric using sklearn.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actual_label : list\n",
    "      Actual(Ground Truth) class label from the dataset.\n",
    "    predicted_label : pd.DataFrame\n",
    "      Class label predicted by the model\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    f1_score : float\n",
    "    accuracy : float\n",
    "    precision : float\n",
    "    recall : float\n",
    "    AUROC : float\n",
    "    '''\n",
    "    precision = metrics.precision_score(actual_label, predicted_label, pos_label=1)\n",
    "    recall = metrics.recall_score(actual_label, predicted_label,pos_label=1)\n",
    "    AUROC = metrics.roc_auc_score(actual_label, predicted_label)\n",
    "    accuracy = metrics.accuracy_score(actual_label, predicted_label)\n",
    "    f1_score = metrics.f1_score(actual_label, predicted_label,pos_label=1)\n",
    "    confusion_mat = metrics.confusion_matrix(actual_label, predicted_label)\n",
    "    metrics_list = [f1_score, accuracy, precision, recall, AUROC]\n",
    "    metrics_list = pd.DataFrame(metrics_list).T\n",
    "    metrics_df = metrics_list.rename(columns={0:'F1',1:'Accuracy',2:'Precision',3:'Recall',4:'AUROC'})\n",
    "    return metrics_df, confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion_mat, model_name):\n",
    "    cm_matplot, cm_ax = plt.subplots(facecolor='#212936')\n",
    "    # cm_ax.text(color='white')\n",
    "    cm_plot = ConfusionMatrixDisplay(confusion_matrix=confusion_mat,display_labels=['not_disaster','disaster'])\n",
    "    title = model_name + \" Confusion Matrix\"\n",
    "    cm_plot.plot(cmap=plt.cm.Greens, ax=cm_ax)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_perf_score(models_list, perf_score_list):\n",
    "  '''Function to consolidate the performance metrics of all the models(KNeighborsClassifier, RandomForestClassifier, LogisticRegression, MLPClassifier) \n",
    "  and return a pd.DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models_list : list\n",
    "      List of models.\n",
    "    perf_score_list : list\n",
    "      List of performance metrics data frame from various models.\n",
    "      \n",
    "    Return\n",
    "    ------\n",
    "    consolidated_metrics_df : pd.DataFrame\n",
    "    '''\n",
    "  \n",
    "  consolidated_perf_score_df = pd.concat(perf_score_list)\n",
    "  consolidated_perf_score_df = consolidated_perf_score_df.rename(columns={0:'F1',1:'Accuracy',2:'Precision',3:'Recall'})\n",
    "  consolidated_perf_score_df.insert(0,'Model',models_list)\n",
    "  return consolidated_perf_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets=train_tweets.to_frame()\n",
    "validation_tweets = validation_tweets.to_frame()\n",
    "test_tweets = test_tweets.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets['processed_text'] = train_tweets['text'].apply(preprocessing)\n",
    "validation_tweets['processed_text'] = validation_tweets['text'].apply(preprocessing)\n",
    "test_tweets['processed_text'] = test_tweets['text'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=18000)\n",
    "tokenizer.fit_on_texts(train_tweets['processed_text'])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_tweets['processed_text'])\n",
    "X_val = tokenizer.texts_to_sequences(validation_tweets['processed_text'])\n",
    "X_test = tokenizer.texts_to_sequences(test_tweets['processed_text'])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "maxlen = 128\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 128, 50)           536250    \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 50)                0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                510       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 537211 (2.05 MB)\n",
      "Trainable params: 537211 (2.05 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "# model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.add(layers.Dense(10, activation='tanh'))\n",
    "model.add(layers.Dense(10, activation='gelu'))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21fd02decd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model = model\n",
    "rnn_model.fit(X_train, train_labels,\n",
    "                    epochs=2,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_val, validation_labels),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pred_test = rnn_model.predict(X_test)\n",
    "# pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.where(pred_test > 0.7, 1, 0)\n",
    "pred_test = pred_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_rnn, confusion_mat_rnn = get_performance_score(test_labels, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, None, 50)          1000000   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 50)          0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, None, 128)         44928     \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, None, 128)         114816    \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Gl  (None, 128)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1176385 (4.49 MB)\n",
      "Trainable params: 1176385 (4.49 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "cnn_layer = layers.Embedding(20000, embedding_dim)(inputs)\n",
    "cnn_layer = layers.Dropout(0.5)(cnn_layer)\n",
    "\n",
    "\n",
    "cnn_layer = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(cnn_layer)\n",
    "# cnn_layer = layers.Conv1D(128, 7, padding=\"valid\", activation=\"softmax\", strides=3)(cnn_layer)\n",
    "# cnn_layer = layers.Conv1D(128, 7, padding=\"valid\", activation=\"softmax\", strides=3)(cnn_layer)\n",
    "cnn_layer = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(cnn_layer)\n",
    "cnn_layer = layers.GlobalMaxPooling1D()(cnn_layer)\n",
    "\n",
    "\n",
    "cnn_layer = layers.Dense(128, activation=\"relu\")(cnn_layer)\n",
    "cnn_layer = layers.Dropout(0.5)(cnn_layer)\n",
    "\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(cnn_layer)\n",
    "\n",
    "cnn_model = keras.Model(inputs, predictions)\n",
    "\n",
    "cnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21fd0518810>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(X_train, train_labels,\n",
    "                    epochs=2,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_val, validation_labels),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_test = cnn_model.predict(X_test)\n",
    "\n",
    "pred_test = np.where(pred_test > 0.7, 1, 0)\n",
    "pred_test = pred_test.flatten()\n",
    "metrics_df_cnn, confusion_mat_cnn = get_performance_score(test_labels, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>AUROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN</td>\n",
       "      <td>0.720370</td>\n",
       "      <td>0.801707</td>\n",
       "      <td>0.913146</td>\n",
       "      <td>0.594801</td>\n",
       "      <td>0.776112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>0.733096</td>\n",
       "      <td>0.803020</td>\n",
       "      <td>0.876596</td>\n",
       "      <td>0.629969</td>\n",
       "      <td>0.781613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model        F1  Accuracy  Precision    Recall     AUROC\n",
       "0   RNN  0.720370  0.801707   0.913146  0.594801  0.776112\n",
       "0   CNN  0.733096  0.803020   0.876596  0.629969  0.781613"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consolidate_perf_score(['RNN', 'CNN'],[metrics_df_rnn, metrics_df_cnn])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
